<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Activation Function in Neural Network | Shawn&#39;s blog</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="icon" href="/img/favicon.ico">
    <script data-ad-client="ca-pub-7828333725993554" async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <meta name="description" content="">
    <meta name="keywords" content="前端博客,个人技术博客,前端,前端开发,前端框架,web前端,前端面试题,技术文档,学习,面试,JavaScript,js,ES6,TypeScript,vue,python,css3,html5,Node,git,github,markdown">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    <link rel="preload" href="/assets/css/0.styles.591622b9.css" as="style"><link rel="preload" href="/assets/js/app.1d409b2f.js" as="script"><link rel="preload" href="/assets/js/2.d36e27d7.js" as="script"><link rel="preload" href="/assets/js/23.3839b11d.js" as="script"><link rel="prefetch" href="/assets/js/10.ed98f359.js"><link rel="prefetch" href="/assets/js/11.6b345c00.js"><link rel="prefetch" href="/assets/js/12.c8fac8f7.js"><link rel="prefetch" href="/assets/js/13.1dd40a92.js"><link rel="prefetch" href="/assets/js/14.459d6bcb.js"><link rel="prefetch" href="/assets/js/15.e1db1f77.js"><link rel="prefetch" href="/assets/js/16.a9e87dcf.js"><link rel="prefetch" href="/assets/js/17.602defa3.js"><link rel="prefetch" href="/assets/js/18.1e4927f6.js"><link rel="prefetch" href="/assets/js/19.0c8b3422.js"><link rel="prefetch" href="/assets/js/20.bfaf8001.js"><link rel="prefetch" href="/assets/js/21.a09c462a.js"><link rel="prefetch" href="/assets/js/22.9f5c8b36.js"><link rel="prefetch" href="/assets/js/24.1d66f58f.js"><link rel="prefetch" href="/assets/js/25.95166e93.js"><link rel="prefetch" href="/assets/js/26.f8fc30ce.js"><link rel="prefetch" href="/assets/js/27.5a6cf34f.js"><link rel="prefetch" href="/assets/js/28.a7c77973.js"><link rel="prefetch" href="/assets/js/29.1c18cbf6.js"><link rel="prefetch" href="/assets/js/3.730b2080.js"><link rel="prefetch" href="/assets/js/30.05293a7d.js"><link rel="prefetch" href="/assets/js/31.20cf2771.js"><link rel="prefetch" href="/assets/js/32.322d22f4.js"><link rel="prefetch" href="/assets/js/33.31ea4206.js"><link rel="prefetch" href="/assets/js/34.d3a8af97.js"><link rel="prefetch" href="/assets/js/35.ca424780.js"><link rel="prefetch" href="/assets/js/36.f281d284.js"><link rel="prefetch" href="/assets/js/37.8e8f52b0.js"><link rel="prefetch" href="/assets/js/38.8da943c4.js"><link rel="prefetch" href="/assets/js/39.49ac1b67.js"><link rel="prefetch" href="/assets/js/4.92defa5c.js"><link rel="prefetch" href="/assets/js/40.22af0611.js"><link rel="prefetch" href="/assets/js/41.0c6ef6aa.js"><link rel="prefetch" href="/assets/js/42.b880683e.js"><link rel="prefetch" href="/assets/js/43.1d69f4c1.js"><link rel="prefetch" href="/assets/js/44.c2edc55f.js"><link rel="prefetch" href="/assets/js/45.78c3807d.js"><link rel="prefetch" href="/assets/js/46.6a0eab44.js"><link rel="prefetch" href="/assets/js/47.c90e0907.js"><link rel="prefetch" href="/assets/js/48.fe922871.js"><link rel="prefetch" href="/assets/js/49.d427c0e1.js"><link rel="prefetch" href="/assets/js/5.c504af6a.js"><link rel="prefetch" href="/assets/js/50.b50fc86c.js"><link rel="prefetch" href="/assets/js/6.74865254.js"><link rel="prefetch" href="/assets/js/7.cee0acd9.js"><link rel="prefetch" href="/assets/js/8.c449cb70.js"><link rel="prefetch" href="/assets/js/9.43999b89.js">
    <link rel="stylesheet" href="/assets/css/0.styles.591622b9.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/images/web-icon.png" alt="Shawn's blog" class="logo"> <span class="site-name can-hide">Shawn's blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/about/about-me.html" class="nav-link">About Me</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Index" class="dropdown-title"><a href="/archives/" class="link-title">Index</a> <span class="title" style="display:none;">Index</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">Category</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">Tag</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">Archive</a></li></ul></div></div> <a href="https://github.com/jinchenxiangdan/myBlog" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="/images/cartoon_icon-removebg.png"> <div class="blogger-info"><h3>Shawn Jin</h3> <span>I am not a creator of knowledge, I am just a porter of knowledge.</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/about/about-me.html" class="nav-link">About Me</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Index" class="dropdown-title"><a href="/archives/" class="link-title">Index</a> <span class="title" style="display:none;">Index</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">Category</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">Tag</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">Archive</a></li></ul></div></div> <a href="https://github.com/jinchenxiangdan/myBlog" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>linear-algebra</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>statistic</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>data-mining</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>machine-learning</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>linear-regression</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>linear-modelling</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>nerual-networks</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/6662bb/" aria-current="page" class="active sidebar-link">Activation Function in Neural Network</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/pages/6662bb/#common-activation-functions" class="sidebar-link">Common Activation Functions</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/pages/6662bb/#_1-sigmoid" class="sidebar-link">1. Sigmoid</a></li><li class="sidebar-sub-header"><a href="/pages/6662bb/#_2-tanh-or-hyperbolic-tangent-activation-function" class="sidebar-link">2.Tanh or hyperbolic tangent Activation Function</a></li><li class="sidebar-sub-header"><a href="/pages/6662bb/#_3-relu-rectified-linear-unit-activation-function" class="sidebar-link">3. ReLU (Rectified Linear Unit) Activation Function</a></li><li class="sidebar-sub-header"><a href="/pages/6662bb/#_4-leaky-relu" class="sidebar-link">4. Leaky ReLU</a></li><li class="sidebar-sub-header"><a href="/pages/6662bb/#_5-softmax" class="sidebar-link">5. Softmax</a></li></ul></li><li class="sidebar-sub-header"><a href="/pages/6662bb/#cheetsheet-of-common-activation-functions" class="sidebar-link">Cheetsheet of Common Activation Functions</a></li><li class="sidebar-sub-header"><a href="/pages/6662bb/#references" class="sidebar-link">References</a></li></ul></li></ul></section></li><li><a href="/pages/6f6732/" class="sidebar-link">Difference between Training set, Validation set and Test Set</a></li><li><a href="/pages/9edd07/" class="sidebar-link">Regularization in Machine Learning</a></li><li><a href="/pages/1de17f/" class="sidebar-link">Learning Feedforward Neural Network Through XOR</a></li><li><a href="/pages/e74889/" class="sidebar-link">Backprop Algorithm in Machine Learning</a></li></ul></section></li><li><a href="/pages/8aa090/" class="sidebar-link">Data Science or Information Science</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-1cd794fe><div class="articleInfo" data-v-1cd794fe><ul class="breadcrumbs" data-v-1cd794fe><li data-v-1cd794fe><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-1cd794fe></a></li> <li data-v-1cd794fe><a href="/categories/?category=Data-Science" title="分类" data-v-1cd794fe>Data-Science</a></li> <li data-v-1cd794fe><a href="/categories/?category=machine-learning" title="分类" data-v-1cd794fe>machine-learning</a></li> <li data-v-1cd794fe><a href="/categories/?category=nerual-networks" title="分类" data-v-1cd794fe>nerual-networks</a></li></ul> <div class="info" data-v-1cd794fe><div title="作者" class="author iconfont icon-touxiang" data-v-1cd794fe><a href="https://github.com/jinchenxiangdan" target="_blank" title="作者" class="beLink" data-v-1cd794fe>Shawn Jin</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-1cd794fe><a href="javascript:;" data-v-1cd794fe>2021-09-14</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">
          Activation Function in Neural Network
        </h1>  <div class="theme-vdoing-content content__default"><h1 id="activation-function-in-neural-network"><a href="#activation-function-in-neural-network" class="header-anchor">#</a> Activation Function in Neural Network</h1> <div class="custom-block note"><p class="custom-block-title">Activation Function</p> <p>It’s just a thing function that you use to get the output of node. It is also known as <strong>Transfer Function</strong>. It could be basically divided into 2 types:</p> <ol><li><p>Linear Activation Function</p></li> <li><p>Non-linear Activation Function</p> <ol><li><p>The main terminologies needed to understand for nonlinear functions are:</p></li> <li><p><strong>Derivative or Differential:</strong> Change in y-axis w.r.t. change in x-axis.It is also known as slope.</p> <p><strong>Monotonic function:</strong> A function which is either entirely non-increasing or non-decreasing.</p></li></ol></li></ol></div> <h3 id="why-we-use-activation-functions-with-neural-networks"><a href="#why-we-use-activation-functions-with-neural-networks" class="header-anchor">#</a> Why we use activation functions with Neural Networks?</h3> <p>It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).</p> <h2 id="common-activation-functions"><a href="#common-activation-functions" class="header-anchor">#</a> Common Activation Functions</h2> <h3 id="_1-sigmoid"><a href="#_1-sigmoid" class="header-anchor">#</a> 1. Sigmoid</h3> <p>The Sigmoid Function curve looks like a S-shape.</p> <p><img src="https://github.com/jinchenxiangdan/myBlog/blob/master/docs/.vuepress/public/images/sigmoid-af.png?raw=true" alt=""></p> <p>The main reason why we use sigmoid function is because it exists between <strong>(0 to 1).</strong> Therefore, it is especially used for models where we have to <strong>predict the probability</strong> as an output. Since probability of anything exists only between the range of <strong>0 and 1,</strong> sigmoid is the right choice.</p> <p>The function is <strong>differentiable</strong>. That means, we can find the slope of the sigmoid curve at any two points.</p> <p>The function is <strong>monotonic</strong> but function’s derivative is not.</p> <p>The <strong>softmax function</strong> is a more generalized logistic activation function which is used for multiclass classification. The logistic sigmoid function can cause a neural network to get stuck at the training time.</p> <h3 id="_2-tanh-or-hyperbolic-tangent-activation-function"><a href="#_2-tanh-or-hyperbolic-tangent-activation-function" class="header-anchor">#</a> 2.Tanh or hyperbolic tangent Activation Function</h3> <p>tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).</p> <p><img src="https://github.com/jinchenxiangdan/myBlog/blob/master/docs/.vuepress/public/images/tanh-af.jpeg?raw=true" alt=""></p> <p>The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.</p> <p>The function is <strong>differentiable</strong>.</p> <p>The function is <strong>monotonic</strong> while its <strong>derivative is not monotonic</strong>.</p> <p>The tanh function is mainly used classification between two classes.</p> <blockquote><p><em>Both tanh and logistic sigmoid activation functions are used in feed-forward nets.</em></p></blockquote> <h3 id="_3-relu-rectified-linear-unit-activation-function"><a href="#_3-relu-rectified-linear-unit-activation-function" class="header-anchor">#</a> 3. ReLU (Rectified Linear Unit) Activation Function</h3> <p>The ReLU is the most used activation function in the world right now. Since, it is used in almost all the convolutional neural networks or deep learning.</p> <p>The function and its derivative <strong>both are</strong> <strong>monotonic</strong>.</p> <p>But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.</p> <h3 id="_4-leaky-relu"><a href="#_4-leaky-relu" class="header-anchor">#</a> 4. Leaky ReLU</h3> <p>It is an attempt to solve the dying ReLU problem.</p> <p><img src="https://github.com/jinchenxiangdan/myBlog/blob/master/docs/.vuepress/public/images/leaky-ReLU.png?raw=true" alt=""></p> <p>The leak helps to increase the range of the ReLU function. Usually, the value of <strong>a</strong> is 0.01 or so.</p> <p>When <strong>a is not 0.01</strong> then it is called <strong>Randomized ReLU</strong>.</p> <p>The <strong>range</strong> of the Leaky ReLU is (-infinity to infinity).</p> <div class="custom-block tip"><p class="custom-block-title">Notes</p> <p>Theoretically, Leaky ReLU has all advances of ReLU and</p></div> <h3 id="_5-softmax"><a href="#_5-softmax" class="header-anchor">#</a> 5. Softmax</h3> <p>Softmax is different with max function.</p> <p><img src="https://github.com/jinchenxiangdan/myBlog/blob/master/docs/.vuepress/public/images/softmax-af.png?raw=true" alt=""></p> <p>The main issue of softmax is it not differentiable at origin.</p> <h2 id="cheetsheet-of-common-activation-functions"><a href="#cheetsheet-of-common-activation-functions" class="header-anchor">#</a> Cheetsheet of Common Activation Functions</h2> <p><img src="https://github.com/jinchenxiangdan/myBlog/blob/master/docs/.vuepress/public/images/activation-function-cheetsheet.png?raw=true" alt=""></p> <h2 id="references"><a href="#references" class="header-anchor">#</a> References</h2> <p><a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" target="_blank" rel="noopener noreferrer">Activation Functions in Neural Networks | by SAGAR SHARMA | Towards Data Science<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://finance.sina.com.cn/tech/2021-02-24/doc-ikftssap8455930.shtml" target="_blank" rel="noopener noreferrer">深度学习领域最常用的10个激活函数，一文详解数学原理及优缺点|深度学习|梯度_新浪科技_新浪网 (sina.com.cn)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div>  <div class="page-edit"><!----> <div class="tags"><a href="/tags/?tag=Neural%20Network" title="标签">#Neural Network</a></div> <div class="last-updated"><span class="prefix">Updated:</span> <span class="time">2021/09/15, 20:43:56</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/769dfe/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Cross-validation</div></a> <a href="/pages/6f6732/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Difference between Training set, Validation set and Test Set</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/769dfe/" class="prev">Cross-validation</a></span> <span class="next"><a href="/pages/6f6732/">Difference between Training set, Validation set and Test Set</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/8e0f46/"><div>Python import files from different directories</div></a> <span>12-31</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/a1eb03/"><div>Classmethod in Python</div></a> <span>09-15</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/15af38/"><div>Single/Double Star (/*) Parameters in Python</div></a> <span>09-15</span></dt></dl> <dl><dd></dd> <dt><a href="/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="mailto:botshawn@qq.com" title="Email" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/jinchenxiangdan" title="GitHub" target="_blank" class="iconfont icon-github"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2019-2021
    <span>Shawn Jin | <a href="https://github.com/xugaoyi/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><div></div></div></div>
    <script src="/assets/js/app.1d409b2f.js" defer></script><script src="/assets/js/2.d36e27d7.js" defer></script><script src="/assets/js/23.3839b11d.js" defer></script>
  </body>
</html>