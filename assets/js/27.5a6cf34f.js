(window.webpackJsonp=window.webpackJsonp||[]).push([[27],{461:function(a,t,e){"use strict";e.r(t);var o=e(15),r=Object(o.a)({},(function(){var a=this,t=a.$createElement,e=a._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"backprop-algorithm-in-ml"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#backprop-algorithm-in-ml"}},[a._v("#")]),a._v(" Backprop Algorithm in ML")]),a._v(" "),e("p",[a._v("Back propagation(Backprop) allows information from the cost to then flow backward through the network in order to compute the gradient.")]),a._v(" "),e("p",[a._v("The term back-propagation is often misunderstood as meaning the whole learning algorithm for multi layer neural networks. Actually, back-propagation refers only to the method for computing the gradient, while another algorithm, such as stochastic gradient descent, is used to perform learning using this gradient.")]),a._v(" "),e("h2",{attrs:{id:"computational-graph"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#computational-graph"}},[a._v("#")]),a._v(" Computational Graph")]),a._v(" "),e("p",[a._v("Many ways of formalizing computation as graphs are possible. Here, we use each node in the graph to indicate a variable. The variable may be a scalar, vector, matrix, tensor, or even a variable of another type.")]),a._v(" "),e("p",[a._v("To formalize our graphs, we also need to introduce the idea of an opera on . An operation is a simple function of one or more variables. Our graph language is accompanied by a set of allowable operations. Functions more complicated than the operations in this set may be described by composing many operations together.")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/jinchenxiangdan/myBlog/blob/master/docs/.vuepress/public/images/compuutational-graphs.png?raw=true",alt:"image-20210908223916027"}})]),a._v(" "),e("h2",{attrs:{id:"chain-rule-of-calculus"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#chain-rule-of-calculus"}},[a._v("#")]),a._v(" Chain Rule of Calculus")]),a._v(" "),e("p",[a._v("The chain rule of calculus (not to be confused with the chain rule of probability) is used to compute the derivatives of functions formed by composing other functions whose derivatives are known. Back-propagation is an algorithm that computes the chain rule, with a speciﬁc order of operations that is highly eﬃcient.")]),a._v(" "),e("div",{staticClass:"custom-block note"},[e("p",{staticClass:"custom-block-title"},[a._v("Jacobian Matrix")]),a._v(" "),e("p",[a._v("A "),e("strong",[a._v("Jacobian matrix")]),a._v(", sometimes simply called a "),e("strong",[a._v("Jacobian")]),a._v(", is a "),e("a",{attrs:{href:"https://math.wikia.org/wiki/Matrix",target:"_blank",rel:"noopener noreferrer"}},[a._v("matrix"),e("OutboundLink")],1),a._v(" of first order "),e("a",{attrs:{href:"https://math.wikia.org/wiki/Partial_derivative",target:"_blank",rel:"noopener noreferrer"}},[a._v("partial derivatives"),e("OutboundLink")],1),a._v(' (in some cases, the term "Jacobian" also refers to the '),e("a",{attrs:{href:"https://math.wikia.org/wiki/Determinant",target:"_blank",rel:"noopener noreferrer"}},[a._v("determinant"),e("OutboundLink")],1),a._v(" of the Jacobian matrix).")])])])}),[],!1,null,null,null);t.default=r.exports}}]);